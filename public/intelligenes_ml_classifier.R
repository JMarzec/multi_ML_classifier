#!/usr/bin/env Rscript
# =============================================================================
# IntelliGenes-Style Multi-Method ML Classifier
# =============================================================================
# Implements various ML methods (RF, SVM, XGBoost, KNN, MLP) with voting
# classifiers, feature selection, and permutation testing for robust
# diagnostic prediction.
#
# Inspired by:
# - IntelliGenes (https://github.com/drzeeshanahmed/intelligenes)
# - Molecular Classification Analysis (https://github.com/CoLAB-AccelBio/molecular-classification-analysis)
# - Li et al. 2022 permutation strategy (https://pubmed.ncbi.nlm.nih.gov/35292087/)
#
# Author: Generated by Lovable AI
# Date: 2025
# =============================================================================

# Load required libraries
suppressPackageStartupMessages({
  library(caret)
  library(randomForest)
  library(e1071)
  library(xgboost)
  library(class)
  library(nnet)
  library(pROC)
  library(jsonlite)
  library(dplyr)
  library(tidyr)
  library(ggplot2)
  library(parallel)
})

# =============================================================================
# CONFIGURATION
# =============================================================================

config <- list(
  # Data settings
  target_variable = "diagnosis",
  id_column = "sample_id",
  
  # Model settings
  seed = 42,
  n_folds = 5,
  n_repeats = 10,
  top_percent = 10,
  
  # Feature selection
  feature_selection_method = "stepwise",  # "forward", "backward", "stepwise", "none"
  max_features = 50,
  
  # Permutation testing
  n_permutations = 100,
  
  # RF specific
  rf_ntree = 500,
  rf_mtry = NULL,  # NULL = auto
  
  # SVM specific
  svm_kernel = "radial",
  svm_cost = 1,
  svm_gamma = NULL,
  
  # XGBoost specific
  xgb_nrounds = 100,
  xgb_max_depth = 6,
  xgb_eta = 0.3,
  
  # KNN specific
  knn_k = 5,
  
  # MLP specific
  mlp_size = 10,
  mlp_decay = 0.01,
  mlp_maxit = 200,
  
  # Output
  output_dir = "./results",
  output_json = "ml_results.json"
)

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

#' Print progress message with timestamp
log_message <- function(msg, level = "INFO") {
  timestamp <- format(Sys.time(), "%Y-%m-%d %H:%M:%S")
  cat(sprintf("[%s] %s: %s\n", timestamp, level, msg))
}

#' Calculate multiple classification metrics
calculate_metrics <- function(actual, predicted, probabilities = NULL) {
  cm <- confusionMatrix(as.factor(predicted), as.factor(actual), positive = "1")
  
  metrics <- list(
    accuracy = as.numeric(cm$overall["Accuracy"]),
    sensitivity = as.numeric(cm$byClass["Sensitivity"]),
    specificity = as.numeric(cm$byClass["Specificity"]),
    precision = as.numeric(cm$byClass["Pos Pred Value"]),
    f1_score = as.numeric(cm$byClass["F1"]),
    balanced_accuracy = as.numeric(cm$byClass["Balanced Accuracy"]),
    kappa = as.numeric(cm$overall["Kappa"])
  )
  
  # Calculate AUROC if probabilities available
  if (!is.null(probabilities)) {
    roc_obj <- roc(actual, probabilities, quiet = TRUE)
    metrics$auroc <- as.numeric(auc(roc_obj))
    metrics$roc_curve <- list(
      specificity = as.numeric(roc_obj$specificities),
      sensitivity = as.numeric(roc_obj$sensitivities)
    )
  }
  
  return(metrics)
}

#' Normalize feature importance scores
normalize_importance <- function(importance) {
  min_val <- min(importance, na.rm = TRUE)
  max_val <- max(importance, na.rm = TRUE)
  if (max_val == min_val) return(rep(1, length(importance)))
  return((importance - min_val) / (max_val - min_val))
}

# =============================================================================
# DATA PREPROCESSING
# =============================================================================

#' Load and preprocess data
load_data <- function(file_path, config) {
  log_message(sprintf("Loading data from: %s", file_path))
  
  # Detect file type and load
  if (grepl("\\.csv$", file_path, ignore.case = TRUE)) {
    data <- read.csv(file_path, stringsAsFactors = FALSE)
  } else if (grepl("\\.tsv$", file_path, ignore.case = TRUE)) {
    data <- read.delim(file_path, stringsAsFactors = FALSE)
  } else if (grepl("\\.rds$", file_path, ignore.case = TRUE)) {
    data <- readRDS(file_path)
  } else {
    stop("Unsupported file format. Use .csv, .tsv, or .rds")
  }
  
  log_message(sprintf("Loaded %d samples with %d features", nrow(data), ncol(data)))
  
  # Store sample IDs if present
  sample_ids <- NULL
  if (config$id_column %in% colnames(data)) {
    sample_ids <- data[[config$id_column]]
    data <- data[, colnames(data) != config$id_column]
  }
  
  # Extract target variable
  if (!config$target_variable %in% colnames(data)) {
    stop(sprintf("Target variable '%s' not found in data", config$target_variable))
  }
  
  y <- as.factor(data[[config$target_variable]])
  X <- data[, colnames(data) != config$target_variable]
  
  # Remove constant columns
  constant_cols <- sapply(X, function(x) length(unique(x)) == 1)
  if (any(constant_cols)) {
    log_message(sprintf("Removing %d constant columns", sum(constant_cols)), "WARN")
    X <- X[, !constant_cols]
  }
  
  # Handle missing values
  if (any(is.na(X))) {
    log_message("Imputing missing values with median", "WARN")
    X <- as.data.frame(lapply(X, function(x) {
      if (is.numeric(x)) {
        x[is.na(x)] <- median(x, na.rm = TRUE)
      }
      return(x)
    }))
  }
  
  # Scale numeric features
  numeric_cols <- sapply(X, is.numeric)
  X[, numeric_cols] <- scale(X[, numeric_cols])
  
  log_message(sprintf("Preprocessed data: %d samples, %d features", nrow(X), ncol(X)))
  
  return(list(
    X = X,
    y = y,
    sample_ids = sample_ids,
    feature_names = colnames(X)
  ))
}

# =============================================================================
# FEATURE SELECTION
# =============================================================================

#' Forward selection
forward_selection <- function(X, y, max_features, seed = 42) {
  set.seed(seed)
  log_message("Performing forward selection...")
  
  selected <- c()
  remaining <- colnames(X)
  best_accuracy <- 0
  
  for (i in 1:min(max_features, length(remaining))) {
    results <- sapply(remaining, function(feat) {
      current_features <- c(selected, feat)
      data_subset <- X[, current_features, drop = FALSE]
      
      # Quick RF evaluation
      ctrl <- trainControl(method = "cv", number = 3)
      model <- train(x = data_subset, y = y, method = "rf",
                    trControl = ctrl, ntree = 100, tuneLength = 1)
      return(max(model$results$Accuracy))
    })
    
    best_feat <- names(which.max(results))
    best_acc <- max(results)
    
    if (best_acc > best_accuracy) {
      best_accuracy <- best_acc
      selected <- c(selected, best_feat)
      remaining <- setdiff(remaining, best_feat)
      log_message(sprintf("  Added feature %d: %s (Acc: %.4f)", i, best_feat, best_acc))
    } else {
      log_message("  No improvement, stopping early")
      break
    }
  }
  
  return(selected)
}

#' Backward elimination
backward_elimination <- function(X, y, min_features = 5, seed = 42) {
  set.seed(seed)
  log_message("Performing backward elimination...")
  
  selected <- colnames(X)
  
  # Initial accuracy
  ctrl <- trainControl(method = "cv", number = 3)
  model <- train(x = X, y = y, method = "rf",
                trControl = ctrl, ntree = 100, tuneLength = 1)
  best_accuracy <- max(model$results$Accuracy)
  
  while (length(selected) > min_features) {
    results <- sapply(selected, function(feat) {
      current_features <- setdiff(selected, feat)
      data_subset <- X[, current_features, drop = FALSE]
      
      model <- train(x = data_subset, y = y, method = "rf",
                    trControl = ctrl, ntree = 100, tuneLength = 1)
      return(max(model$results$Accuracy))
    })
    
    worst_feat <- names(which.max(results))
    acc_without <- max(results)
    
    if (acc_without >= best_accuracy - 0.01) {
      selected <- setdiff(selected, worst_feat)
      best_accuracy <- acc_without
      log_message(sprintf("  Removed: %s (Acc: %.4f, %d features remaining)", 
                         worst_feat, acc_without, length(selected)))
    } else {
      log_message("  No improvement from removing features, stopping")
      break
    }
  }
  
  return(selected)
}

#' Stepwise selection (forward with pruning)
stepwise_selection <- function(X, y, max_features, seed = 42) {
  set.seed(seed)
  log_message("Performing stepwise selection...")
  
  selected <- c()
  remaining <- colnames(X)
  best_accuracy <- 0
  ctrl <- trainControl(method = "cv", number = 3)
  
  for (i in 1:min(max_features, length(remaining))) {
    # Forward step
    results <- sapply(remaining, function(feat) {
      current_features <- c(selected, feat)
      data_subset <- X[, current_features, drop = FALSE]
      model <- train(x = data_subset, y = y, method = "rf",
                    trControl = ctrl, ntree = 100, tuneLength = 1)
      return(max(model$results$Accuracy))
    })
    
    best_feat <- names(which.max(results))
    best_acc <- max(results)
    
    if (best_acc > best_accuracy) {
      selected <- c(selected, best_feat)
      remaining <- setdiff(remaining, best_feat)
      best_accuracy <- best_acc
      log_message(sprintf("  Added: %s (Acc: %.4f)", best_feat, best_acc))
      
      # Backward step (try removing each selected feature except the new one)
      if (length(selected) > 2) {
        for (feat in selected[-length(selected)]) {
          test_features <- setdiff(selected, feat)
          data_subset <- X[, test_features, drop = FALSE]
          model <- train(x = data_subset, y = y, method = "rf",
                        trControl = ctrl, ntree = 100, tuneLength = 1)
          acc <- max(model$results$Accuracy)
          
          if (acc >= best_accuracy) {
            selected <- test_features
            best_accuracy <- acc
            remaining <- c(remaining, feat)
            log_message(sprintf("  Removed: %s (Acc: %.4f)", feat, acc))
          }
        }
      }
    } else {
      break
    }
  }
  
  return(selected)
}

#' Main feature selection wrapper
perform_feature_selection <- function(X, y, method, max_features, seed = 42) {
  switch(method,
    "forward" = forward_selection(X, y, max_features, seed),
    "backward" = backward_elimination(X, y, max_features, seed),
    "stepwise" = stepwise_selection(X, y, max_features, seed),
    "none" = colnames(X)
  )
}

# =============================================================================
# MODEL TRAINING FUNCTIONS
# =============================================================================

#' Train Random Forest model
train_rf <- function(X_train, y_train, config) {
  mtry <- config$rf_mtry
  if (is.null(mtry)) {
    mtry <- floor(sqrt(ncol(X_train)))
  }
  
  model <- randomForest(
    x = X_train,
    y = y_train,
    ntree = config$rf_ntree,
    mtry = mtry,
    importance = TRUE
  )
  
  # Extract importance
  importance <- importance(model, type = 2)[, 1]
  
  return(list(
    model = model,
    importance = importance,
    oob_error = model$err.rate[config$rf_ntree, "OOB"]
  ))
}

#' Train SVM model
train_svm <- function(X_train, y_train, config) {
  gamma <- config$svm_gamma
  if (is.null(gamma)) {
    gamma <- 1 / ncol(X_train)
  }
  
  model <- svm(
    x = as.matrix(X_train),
    y = y_train,
    kernel = config$svm_kernel,
    cost = config$svm_cost,
    gamma = gamma,
    probability = TRUE
  )
  
  return(list(model = model))
}

#' Train XGBoost model
train_xgboost <- function(X_train, y_train, config) {
  # Convert to numeric matrix
  X_matrix <- as.matrix(X_train)
  y_numeric <- as.numeric(y_train) - 1
  
  dtrain <- xgb.DMatrix(data = X_matrix, label = y_numeric)
  
  params <- list(
    objective = "binary:logistic",
    eval_metric = "auc",
    max_depth = config$xgb_max_depth,
    eta = config$xgb_eta
  )
  
  model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = config$xgb_nrounds,
    verbose = 0
  )
  
  # Extract importance
  importance <- xgb.importance(model = model)
  
  return(list(
    model = model,
    importance = importance
  ))
}

#' Train KNN model (returns training data for lazy evaluation)
train_knn <- function(X_train, y_train, config) {
  return(list(
    X_train = X_train,
    y_train = y_train,
    k = config$knn_k
  ))
}

#' Train MLP (Neural Network) model
train_mlp <- function(X_train, y_train, config) {
  model <- nnet(
    x = as.matrix(X_train),
    y = class.ind(y_train),
    size = config$mlp_size,
    decay = config$mlp_decay,
    maxit = config$mlp_maxit,
    softmax = TRUE,
    trace = FALSE
  )
  
  return(list(model = model))
}

# =============================================================================
# PREDICTION FUNCTIONS
# =============================================================================

#' Get predictions and probabilities from RF
predict_rf <- function(model_obj, X_test) {
  pred <- predict(model_obj$model, X_test)
  prob <- predict(model_obj$model, X_test, type = "prob")[, "1"]
  return(list(predictions = pred, probabilities = prob))
}

#' Get predictions and probabilities from SVM
predict_svm <- function(model_obj, X_test) {
  pred <- predict(model_obj$model, as.matrix(X_test))
  prob_attr <- predict(model_obj$model, as.matrix(X_test), probability = TRUE)
  prob <- attr(prob_attr, "probabilities")[, "1"]
  return(list(predictions = pred, probabilities = prob))
}

#' Get predictions and probabilities from XGBoost
predict_xgboost <- function(model_obj, X_test) {
  X_matrix <- as.matrix(X_test)
  prob <- predict(model_obj$model, X_matrix)
  pred <- factor(ifelse(prob > 0.5, "1", "0"), levels = c("0", "1"))
  return(list(predictions = pred, probabilities = prob))
}

#' Get predictions from KNN
predict_knn <- function(model_obj, X_test) {
  pred <- knn(
    train = model_obj$X_train,
    test = X_test,
    cl = model_obj$y_train,
    k = model_obj$k,
    prob = TRUE
  )
  prob <- attr(pred, "prob")
  # Adjust probability based on predicted class
  prob <- ifelse(pred == "1", prob, 1 - prob)
  return(list(predictions = pred, probabilities = prob))
}

#' Get predictions from MLP
predict_mlp <- function(model_obj, X_test) {
  prob_matrix <- predict(model_obj$model, as.matrix(X_test))
  prob <- prob_matrix[, 2]
  pred <- factor(ifelse(prob > 0.5, "1", "0"), levels = c("0", "1"))
  return(list(predictions = pred, probabilities = prob))
}

# =============================================================================
# ENSEMBLE VOTING
# =============================================================================

#' Hard voting classifier
hard_voting <- function(predictions_list) {
  # Convert to matrix
  pred_matrix <- do.call(cbind, lapply(predictions_list, as.character))
  
  # Majority vote
  final_pred <- apply(pred_matrix, 1, function(row) {
    names(which.max(table(row)))
  })
  
  return(factor(final_pred, levels = c("0", "1")))
}

#' Soft voting classifier
soft_voting <- function(probabilities_list, weights = NULL) {
  n_models <- length(probabilities_list)
  
  if (is.null(weights)) {
    weights <- rep(1/n_models, n_models)
  }
  
  # Weighted average
  avg_prob <- Reduce(`+`, Map(`*`, probabilities_list, weights))
  
  pred <- factor(ifelse(avg_prob > 0.5, "1", "0"), levels = c("0", "1"))
  
  return(list(predictions = pred, probabilities = avg_prob))
}

# =============================================================================
# CROSS-VALIDATION WITH MULTIPLE METHODS
# =============================================================================

#' Run complete cross-validation with all methods
run_cv_all_methods <- function(X, y, config, selected_features = NULL) {
  set.seed(config$seed)
  
  if (!is.null(selected_features)) {
    X <- X[, selected_features, drop = FALSE]
  }
  
  n_samples <- nrow(X)
  
  # Create fold indices
  folds <- createMultiFolds(y, k = config$n_folds, times = config$n_repeats)
  
  # Initialize results storage
  all_results <- list(
    rf = list(), svm = list(), xgboost = list(), 
    knn = list(), mlp = list(), 
    hard_vote = list(), soft_vote = list()
  )
  
  feature_importance <- data.frame(feature = colnames(X))
  importance_scores <- list()
  
  log_message(sprintf("Running %d-fold CV with %d repeats (%d total iterations)",
                     config$n_folds, config$n_repeats, length(folds)))
  
  for (i in seq_along(folds)) {
    train_idx <- folds[[i]]
    test_idx <- setdiff(1:n_samples, train_idx)
    
    X_train <- X[train_idx, , drop = FALSE]
    X_test <- X[test_idx, , drop = FALSE]
    y_train <- y[train_idx]
    y_test <- y[test_idx]
    
    if (i %% 10 == 0) {
      log_message(sprintf("  Fold %d/%d", i, length(folds)))
    }
    
    # Train all models
    models <- list(
      rf = tryCatch(train_rf(X_train, y_train, config), error = function(e) NULL),
      svm = tryCatch(train_svm(X_train, y_train, config), error = function(e) NULL),
      xgboost = tryCatch(train_xgboost(X_train, y_train, config), error = function(e) NULL),
      knn = tryCatch(train_knn(X_train, y_train, config), error = function(e) NULL),
      mlp = tryCatch(train_mlp(X_train, y_train, config), error = function(e) NULL)
    )
    
    # Get predictions
    preds <- list()
    probs <- list()
    
    if (!is.null(models$rf)) {
      result <- predict_rf(models$rf, X_test)
      preds$rf <- result$predictions
      probs$rf <- result$probabilities
      all_results$rf[[i]] <- calculate_metrics(y_test, preds$rf, probs$rf)
      
      # Store importance
      if (!is.null(models$rf$importance)) {
        importance_scores[[length(importance_scores) + 1]] <- models$rf$importance
      }
    }
    
    if (!is.null(models$svm)) {
      result <- predict_svm(models$svm, X_test)
      preds$svm <- result$predictions
      probs$svm <- result$probabilities
      all_results$svm[[i]] <- calculate_metrics(y_test, preds$svm, probs$svm)
    }
    
    if (!is.null(models$xgboost)) {
      result <- predict_xgboost(models$xgboost, X_test)
      preds$xgboost <- result$predictions
      probs$xgboost <- result$probabilities
      all_results$xgboost[[i]] <- calculate_metrics(y_test, preds$xgboost, probs$xgboost)
    }
    
    if (!is.null(models$knn)) {
      result <- predict_knn(models$knn, X_test)
      preds$knn <- result$predictions
      probs$knn <- result$probabilities
      all_results$knn[[i]] <- calculate_metrics(y_test, preds$knn, probs$knn)
    }
    
    if (!is.null(models$mlp)) {
      result <- predict_mlp(models$mlp, X_test)
      preds$mlp <- result$predictions
      probs$mlp <- result$probabilities
      all_results$mlp[[i]] <- calculate_metrics(y_test, preds$mlp, probs$mlp)
    }
    
    # Ensemble predictions
    if (length(preds) > 1) {
      # Hard voting
      hard_pred <- hard_voting(preds)
      all_results$hard_vote[[i]] <- calculate_metrics(y_test, hard_pred)
      
      # Soft voting
      soft_result <- soft_voting(probs)
      all_results$soft_vote[[i]] <- calculate_metrics(y_test, soft_result$predictions, 
                                                       soft_result$probabilities)
    }
  }
  
  # Aggregate importance scores
  if (length(importance_scores) > 0) {
    avg_importance <- rowMeans(do.call(cbind, importance_scores), na.rm = TRUE)
    feature_importance$importance <- avg_importance
    feature_importance <- feature_importance[order(-feature_importance$importance), ]
  }
  
  return(list(
    results = all_results,
    feature_importance = feature_importance
  ))
}

# =============================================================================
# PERMUTATION TESTING
# =============================================================================

#' Run permutation testing for model validation
run_permutation_test <- function(X, y, config, selected_features = NULL, 
                                  n_permutations = NULL) {
  if (is.null(n_permutations)) {
    n_permutations <- config$n_permutations
  }
  
  log_message(sprintf("Running permutation testing with %d permutations", n_permutations))
  
  if (!is.null(selected_features)) {
    X <- X[, selected_features, drop = FALSE]
  }
  
  permutation_results <- list(
    rf_oob_error = numeric(n_permutations),
    rf_auroc = numeric(n_permutations),
    ensemble_auroc = numeric(n_permutations)
  )
  
  # Create folds once
  set.seed(config$seed)
  folds <- createFolds(y, k = config$n_folds)
  
  for (p in 1:n_permutations) {
    if (p %% 10 == 0) {
      log_message(sprintf("  Permutation %d/%d", p, n_permutations))
    }
    
    # Shuffle labels
    y_permuted <- sample(y)
    
    # Train RF with permuted data
    rf_model <- tryCatch({
      randomForest(
        x = X,
        y = y_permuted,
        ntree = min(config$rf_ntree, 200),  # Fewer trees for speed
        importance = FALSE
      )
    }, error = function(e) NULL)
    
    if (!is.null(rf_model)) {
      permutation_results$rf_oob_error[p] <- rf_model$err.rate[nrow(rf_model$err.rate), "OOB"]
    }
    
    # Quick CV for AUROC
    cv_probs <- numeric(length(y))
    for (fold in folds) {
      train_idx <- setdiff(1:length(y), fold)
      
      rf_fold <- tryCatch({
        randomForest(x = X[train_idx, ], y = y_permuted[train_idx], 
                    ntree = 100, importance = FALSE)
      }, error = function(e) NULL)
      
      if (!is.null(rf_fold)) {
        cv_probs[fold] <- predict(rf_fold, X[fold, ], type = "prob")[, "1"]
      }
    }
    
    if (any(cv_probs > 0)) {
      roc_obj <- tryCatch({
        roc(y_permuted, cv_probs, quiet = TRUE)
      }, error = function(e) NULL)
      
      if (!is.null(roc_obj)) {
        permutation_results$rf_auroc[p] <- as.numeric(auc(roc_obj))
      }
    }
  }
  
  return(permutation_results)
}

# =============================================================================
# TOP PROFILE RANKING
# =============================================================================

#' Rank samples by prediction confidence and identify top profiles
rank_profiles <- function(X, y, models, config) {
  log_message("Ranking profiles by prediction confidence...")
  
  # Get ensemble probabilities for all samples
  all_probs <- list()
  
  if (!is.null(models$rf)) {
    all_probs$rf <- predict(models$rf$model, X, type = "prob")[, "1"]
  }
  
  if (!is.null(models$svm)) {
    svm_pred <- predict(models$svm$model, as.matrix(X), probability = TRUE)
    all_probs$svm <- attr(svm_pred, "probabilities")[, "1"]
  }
  
  if (!is.null(models$xgboost)) {
    all_probs$xgboost <- predict(models$xgboost$model, as.matrix(X))
  }
  
  if (!is.null(models$mlp)) {
    prob_matrix <- predict(models$mlp$model, as.matrix(X))
    all_probs$mlp <- prob_matrix[, 2]
  }
  
  # Ensemble average
  avg_prob <- rowMeans(do.call(cbind, all_probs), na.rm = TRUE)
  
  # Calculate confidence (distance from 0.5)
  confidence <- abs(avg_prob - 0.5) * 2
  
  # Create ranking
  ranking <- data.frame(
    sample_index = 1:nrow(X),
    actual_class = as.character(y),
    ensemble_probability = avg_prob,
    predicted_class = ifelse(avg_prob > 0.5, "1", "0"),
    confidence = confidence,
    correct = as.character(y) == ifelse(avg_prob > 0.5, "1", "0")
  )
  
  ranking <- ranking[order(-ranking$confidence), ]
  ranking$rank <- 1:nrow(ranking)
  
  # Identify top profiles
  top_n <- ceiling(nrow(ranking) * (config$top_percent / 100))
  ranking$top_profile <- ranking$rank <= top_n
  
  return(ranking)
}

# =============================================================================
# SINGLE PATIENT PREDICTION
# =============================================================================

#' Predict diagnosis for a single new patient
predict_single_patient <- function(patient_data, models, feature_names, config) {
  log_message("Predicting diagnosis for single patient...")
  
  # Ensure patient data has correct features
  patient_data <- patient_data[, feature_names, drop = FALSE]
  
  predictions <- list()
  probabilities <- list()
  
  # RF prediction
  if (!is.null(models$rf)) {
    rf_result <- predict_rf(models$rf, patient_data)
    predictions$rf <- as.character(rf_result$predictions)
    probabilities$rf <- rf_result$probabilities
  }
  
  # SVM prediction
  if (!is.null(models$svm)) {
    svm_result <- predict_svm(models$svm, patient_data)
    predictions$svm <- as.character(svm_result$predictions)
    probabilities$svm <- svm_result$probabilities
  }
  
  # XGBoost prediction
  if (!is.null(models$xgboost)) {
    xgb_result <- predict_xgboost(models$xgboost, patient_data)
    predictions$xgboost <- as.character(xgb_result$predictions)
    probabilities$xgboost <- xgb_result$probabilities
  }
  
  # MLP prediction
  if (!is.null(models$mlp)) {
    mlp_result <- predict_mlp(models$mlp, patient_data)
    predictions$mlp <- as.character(mlp_result$predictions)
    probabilities$mlp <- mlp_result$probabilities
  }
  
  # Ensemble
  if (length(predictions) > 1) {
    hard_pred <- names(which.max(table(unlist(predictions))))
    soft_result <- soft_voting(probabilities)
    
    return(list(
      individual_predictions = predictions,
      individual_probabilities = probabilities,
      hard_vote_prediction = hard_pred,
      soft_vote_prediction = as.character(soft_result$predictions),
      soft_vote_probability = soft_result$probabilities,
      confidence = abs(soft_result$probabilities - 0.5) * 2
    ))
  }
  
  return(list(
    individual_predictions = predictions,
    individual_probabilities = probabilities
  ))
}

# =============================================================================
# RESULTS AGGREGATION
# =============================================================================

#' Aggregate CV results into summary statistics
aggregate_results <- function(results_list) {
  methods <- names(results_list)
  
  summary <- lapply(methods, function(method) {
    if (length(results_list[[method]]) == 0) {
      return(NULL)
    }
    
    metrics <- c("accuracy", "sensitivity", "specificity", "precision", 
                "f1_score", "balanced_accuracy", "auroc", "kappa")
    
    stats <- lapply(metrics, function(m) {
      values <- sapply(results_list[[method]], function(r) r[[m]])
      values <- values[!is.na(values)]
      
      if (length(values) == 0) return(NULL)
      
      list(
        mean = mean(values),
        sd = sd(values),
        median = median(values),
        q25 = quantile(values, 0.25),
        q75 = quantile(values, 0.75),
        min = min(values),
        max = max(values)
      )
    })
    names(stats) <- metrics
    
    return(stats)
  })
  names(summary) <- methods
  
  return(summary)
}

# =============================================================================
# JSON OUTPUT
# =============================================================================

#' Export all results to JSON format
export_to_json <- function(results, config, output_path) {
  log_message(sprintf("Exporting results to: %s", output_path))
  
  output <- list(
    metadata = list(
      generated_at = format(Sys.time(), "%Y-%m-%dT%H:%M:%S"),
      config = config,
      r_version = R.version.string
    ),
    model_performance = results$summary,
    feature_importance = if (!is.null(results$feature_importance)) {
      head(results$feature_importance, 50)
    } else NULL,
    permutation_testing = if (!is.null(results$permutation)) {
      list(
        rf_oob_error = list(
          permuted_mean = mean(results$permutation$rf_oob_error, na.rm = TRUE),
          permuted_sd = sd(results$permutation$rf_oob_error, na.rm = TRUE),
          original = results$original_metrics$rf_oob_error,
          p_value = mean(results$permutation$rf_oob_error <= 
                        results$original_metrics$rf_oob_error, na.rm = TRUE)
        ),
        rf_auroc = list(
          permuted_mean = mean(results$permutation$rf_auroc, na.rm = TRUE),
          permuted_sd = sd(results$permutation$rf_auroc, na.rm = TRUE),
          original = results$original_metrics$rf_auroc,
          p_value = mean(results$permutation$rf_auroc >= 
                        results$original_metrics$rf_auroc, na.rm = TRUE)
        )
      )
    } else NULL,
    profile_ranking = if (!is.null(results$ranking)) {
      list(
        top_profiles = results$ranking[results$ranking$top_profile, ],
        all_rankings = results$ranking
      )
    } else NULL,
    selected_features = results$selected_features
  )
  
  # Write JSON
  json_output <- toJSON(output, auto_unbox = TRUE, pretty = TRUE, digits = 6)
  writeLines(json_output, output_path)
  
  log_message("Results exported successfully")
  
  return(invisible(output_path))
}

# =============================================================================
# MAIN PIPELINE
# =============================================================================

#' Run the complete ML classification pipeline
run_pipeline <- function(data_path, config = NULL, new_patient_data = NULL) {
  if (is.null(config)) {
    config <- config  # Use global config
  }
  
  start_time <- Sys.time()
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  log_message("Starting IntelliGenes-Style ML Classification Pipeline")
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  
  # Create output directory
  if (!dir.exists(config$output_dir)) {
    dir.create(config$output_dir, recursive = TRUE)
  }
  
  # Load data
  data <- load_data(data_path, config)
  
  # Feature selection
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  log_message("FEATURE SELECTION")
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  
  selected_features <- perform_feature_selection(
    data$X, data$y, 
    config$feature_selection_method, 
    config$max_features,
    config$seed
  )
  log_message(sprintf("Selected %d features", length(selected_features)))
  
  # Cross-validation with all methods
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  log_message("CROSS-VALIDATION")
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  
  cv_results <- run_cv_all_methods(data$X, data$y, config, selected_features)
  
  # Aggregate results
  summary <- aggregate_results(cv_results$results)
  
  # Train final models on full data for prediction
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  log_message("TRAINING FINAL MODELS")
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  
  X_selected <- data$X[, selected_features, drop = FALSE]
  
  final_models <- list(
    rf = tryCatch(train_rf(X_selected, data$y, config), error = function(e) NULL),
    svm = tryCatch(train_svm(X_selected, data$y, config), error = function(e) NULL),
    xgboost = tryCatch(train_xgboost(X_selected, data$y, config), error = function(e) NULL),
    knn = tryCatch(train_knn(X_selected, data$y, config), error = function(e) NULL),
    mlp = tryCatch(train_mlp(X_selected, data$y, config), error = function(e) NULL)
  )
  
  # Get original RF metrics for permutation comparison
  original_metrics <- list(
    rf_oob_error = if (!is.null(final_models$rf)) final_models$rf$oob_error else NA,
    rf_auroc = if (!is.null(summary$rf$auroc)) summary$rf$auroc$mean else NA
  )
  
  # Permutation testing
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  log_message("PERMUTATION TESTING")
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  
  permutation_results <- run_permutation_test(
    data$X, data$y, config, selected_features
  )
  
  # Profile ranking
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  log_message("PROFILE RANKING")
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  
  ranking <- rank_profiles(X_selected, data$y, final_models, config)
  
  # Single patient prediction (if provided)
  single_patient_result <- NULL
  if (!is.null(new_patient_data)) {
    single_patient_result <- predict_single_patient(
      new_patient_data, final_models, selected_features, config
    )
  }
  
  # Compile all results
  all_results <- list(
    summary = summary,
    cv_results = cv_results$results,
    feature_importance = cv_results$feature_importance,
    permutation = permutation_results,
    original_metrics = original_metrics,
    ranking = ranking,
    selected_features = selected_features,
    single_patient = single_patient_result
  )
  
  # Export to JSON
  output_path <- file.path(config$output_dir, config$output_json)
  export_to_json(all_results, config, output_path)
  
  end_time <- Sys.time()
  duration <- difftime(end_time, start_time, units = "mins")
  
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  log_message(sprintf("Pipeline completed in %.2f minutes", duration))
  log_message("=" %>% rep(60) %>% paste(collapse = ""))
  
  return(all_results)
}

# =============================================================================
# EXAMPLE USAGE
# =============================================================================

if (FALSE) {
  # Example 1: Run with default config
  results <- run_pipeline("path/to/your/data.csv")
  
  # Example 2: Custom configuration
  my_config <- config
  my_config$feature_selection_method <- "forward"
  my_config$n_permutations <- 50
  my_config$top_percent <- 15
  results <- run_pipeline("data.csv", my_config)
  
  # Example 3: Predict for new patient
  new_patient <- data.frame(
    feature1 = 1.5,
    feature2 = -0.3,
    feature3 = 2.1
    # ... add all required features
  )
  results <- run_pipeline("data.csv", config, new_patient_data = new_patient)
  
  # The results JSON can be uploaded to the dashboard
}

# Print usage instructions
cat("\n")
cat("=" %>% rep(60) %>% paste(collapse = ""), "\n")
cat("IntelliGenes-Style ML Classifier\n")
cat("=" %>% rep(60) %>% paste(collapse = ""), "\n")
cat("\nUsage:\n")
cat("  1. Modify the 'config' list at the top of this script\n")
cat("  2. Run: results <- run_pipeline('your_data.csv')\n")
cat("  3. Upload results JSON to the dashboard\n")
cat("\nRequired packages:\n")
cat("  caret, randomForest, e1071, xgboost, class, nnet,\n")
cat("  pROC, jsonlite, dplyr, tidyr, ggplot2\n")
cat("\nInstall with:\n")
cat("  install.packages(c('caret', 'randomForest', 'e1071', 'xgboost',\n")
cat("                     'class', 'nnet', 'pROC', 'jsonlite',\n")
cat("                     'dplyr', 'tidyr', 'ggplot2'))\n")
cat("=" %>% rep(60) %>% paste(collapse = ""), "\n")
